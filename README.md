# What-DNNs-Forget-SimCLR

For my master thesis I’m [working](https://graz.pure.elsevier.com/en/persons/francesco-corti) since October 2021 at the [Institute of Technical Informatics](https://www.tugraz.at/en/institutes/iti/home/) of the [Graz University of Technology](https://www.tugraz.at/home/) under the supervision of [Prof. Olga Saukh](http://www.olgasaukh.com/), [Rahim Entezari](https://rahimentezari.github.io/) and co-supervised by [Prof. Davide Bacciu](http://pages.di.unipi.it/bacciu/) (University of Pisa) about understanding what is lost in pruned Convolutional Neural Networks trained with [Self-Supervised Learning](https://arxiv.org/pdf/2002.05709.pdf) (SimCLR). My work consists of analysing the performance of the [ResNet](https://arxiv.org/pdf/1512.03385.pdf) and [WideResNet](https://arxiv.org/pdf/1605.07146.pdf) CNN models trained for image classification (currently only on [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html)) after being pruned with [Magnitude Pruning](https://arxiv.org/pdf/1710.01878.pdf). I’ve rewritten the code of the paper [“What Do Compressed Deep Neural Networks Forget?”](https://arxiv.org/abs/1911.05248) firstly in Tensorflow 2.7, then I ported the code to Pytorch 1.10 and I've developed the gradual pruning algorithm described in the magnitude pruning paper. The train and the analysis of the models obtained is done on a server of Nvidia Tesla K80 GPUs. Take a look at the python files and the logs files to have an insight of the work. See the official [description file](https://github.com/FraCorti/What-DNNs-Forget-SimCLR/blob/master/2021_What_Is_Lost_in_Compressed_Unsupervised_Trained_Models_.pdf) of the thesis to have an in-depth theoretical description of the research.  